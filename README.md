# hype_prediction_ehr

This project deals with the prediction of incident hypertension from EHR data. 

**Project Organization**

We used Jupyter Notebooks with Python to run and document our code. 
Our project consists of three main folders:

- Cohorts: Extracted features and final datasets in .parquet files, which must not be pushed to git. The folder is located here: /home/cremej01/hype_prediction_ehr/Cohorts
- Fiber: Documentation and Examples of the framework to extract Patient data 
- Notebooks: All jupyter notebooks used during the project, including feature extraction, machine learning, and optimization

A detailed view of the folder structure can be found in the following file: [Project structure](Project structure.txt)


**Feature Extraction Pipeline Usage Guide**
To use this pipeline, the  cohorts (Case and Control) with demographic data must already exist. 
The Notebook contains three generic funtions: 

1. data frame to cohort: 
```
def df_to_cohort(df):
    mrns = list(df.index.values)
    condition = MRNs(mrns)
    return Cohort(condition)
```
This functions takes a panda dataframe as an input and creates a cohort, which only consists out of the MRNs which are in the dataframe<br>
The input is thereby a dataframe, which contains at least MRNs.

2. Extrating a certain condition: 
```
def get_has_certain_condition(condition, df_mrn, name_feature, gap_in_days, frequency, cohort_type):
    #get cohort
    Onset_column = "HT_Onset" if cohort_type == "Case" else "last_encounter"
    cohort = df_to_cohort(df_mrn)
    ...
```
This funtion is extracting a FIBER Condition for a certain cohort. The input parameters will be explained by the "get_features" function 
3. Get Feature Funtion
```
def get_features(case, control, features, gap_in_days, frequency):
    # case 
    for feature in features:
    ...
``` 
This is the function which must be performed by the user of the notebook. <br>
It requires the following inputs: <br>
* case = case cohort as a  pandas dataframe
* control = control cohort as a  pandas dataframe
* features = an Array which contains touple out of a FIBER condition and the name of the condition for the output dataframe
* gap_in_days= Gap in days before the exposure of a patient. It has to be an integer
* frequency = Â´
  *   EVER: The condition appears at least one time (The Feature will get the Prefix "has_condition")
  *   COUNT: Returns the count of occureces of the condition (The Feature will get the Prefix "number_of_occurences_")
  *   WINDOW:  Returns the count of occureces of the condition for a certain TimeFrame (ATTENTION: This function was never used by the project team)

The function itself is saving an parquet file for each condition seperatly. <br>
The name of the parquet file is thereby generated automatically: 
```
 df_name= (name_feature+"_"+cohort_type+"_"+frequency+"_"+str(gap_in_days)+"_gap_in_days") 
```

**Merge Dataframes Pipeline Usage Guide**
The main function of this Pipeline is to merge differenet dataframes, which are containing only one feature to one big dataframe, which could be used to train Machinelearning models.
This Pipeline take as input the files, which were generated by the *"Feature Extraction Pipeline"*. 
This is the function which must be performed by the user of the notebook: 
```
 def merge_dataframes ( feature_names,case_cohort_path,control_cohort_path, final_dataframe_name, gap):
    #read in case data frame
    case_main_frame=pq.read_table(case_cohort_path).to_pandas()
    ...
```
It requires the following inputs: <br>
* features_name = an array which contains the different feature names, that should be merged (Same names as already defined in the Feature Extraction Pipeline )
* case_cohort_path= Path to the parquet file of the case cohort without any further features  
* control_cohort_path= Path to the parquet file of the control cohort without any further features  
* final_dataframe_name= Final name of the output dataframe(WITHOUT .parquet)
* gap= gap in days as a string, this is needed, to build the name of the parquet file automatically

The function itself is saving one parquet file. 


**Unsupervised Pipeline Usage Guide**

1.  Config PIVOT
Possible for 
* Laboratory values
* Drugs
* Diseases
* Vital Signs
* Procedure

Example config for Drugs, 
adjustable time window from when to start taking the features into consideration, threshold: how many events you want to be present in your cohort 

```
DEFAULT_PIVOT_CONFIG = { Drug(): {
'window':(-math.inf, -180), 'pivot_table_kwargs': {
'columns': ['test_name'],
'aggfunc': {'numeric_value': ['min', 'median', 'max']} }, 'threshold': 0.5
    }
}
```


2.	Loading cohort as dataframe 
* First create a dataframe with the needed Cohort-dataframe and set the time-relevant column as age_in_days
* Set the medical record number as column and not as index
* The dateframe needs three columns: index, medical record number, *'age_in_days'*


3.	Running in batches
*  Due to quickly running out of memory having a large cohort, we split our cohort and ran the code in batches of 50.000 medical record numbers each round and saved those as parquet files to then be concatenated on the same columns

```
for limit in range (0, len(Cohort-dataframe), 50000):
    print("Begin of iteration: " +  str(limit))
    temp = Cohort-dataframe[limit:(limit+50000)]
    p_condition = MRNs(temp) #how to create cohort from dataframe
    cohort = Cohort(p_condition)
    result = cohort.get_pivoted_features(pivot_config=DEFAULT_PIVOT_CONFIG)
    result.to_parquet('Cohorts/Diseases_Cases_' + str(limit))
	
```

**ML Script Pipeline Usage Guide**

The script allows the training and evaluation of a data set using different machine learning models respectivly:
- Logistic Regression
- Random Forest
- XGBoost
- LightGBM
- Multilayer Perceptron

Addionally, the hyperparameter can be optimized using GridSearch or HyperOpt. It can also calculate the importance of every used feature.

1.  To add a new data set to the Script, please add it to the cell #Load and Modify Data# as an Pandas DataFrame. For example:

```
To_train_80k = pq.read_table("Cohorts/ML/Cleaned_Final_Model_BMI_BP_Pulse_4CV_5Drugs_wo_NaN.parquet").to_pandas()
```

2.  All further settings can be done in the next cell #Parameter Setting#:

- To_train: Choose a data set (depending on the set defined in the cell before)
- model: Choose between: 'lr' for Logistic Regression, 'rf' for Random Forest, 'xgb' for XGBoost, 'lgbm' for LightGBM or 'mlp' for Multilayer Perceptron
- GS: Set True if GridSearch should be applied to find the best hyperparameters
- HO: Set True if HyperOpt should be applied to find the best hyperparameters
- Set booth to False if predefined hyperparameter should be used
```
# Choose Data Set

To_train = To_train_80k


# Model Selection
model = 'lr'

# Use Hyperparameter Optimazation
GS = True
HO = False 
```

To set specific parameters:
```
# Predefined Parameters

if (model == 'lr'):
    classifier = LogisticRegression()
    param = {'C': 1.2, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None,
             'multi_class': 'auto', 'penalty': 'l2', 'random_state': None, 'solver': 'newton-cg', 'tol': 0.0001, 'verbose': 0,
             'warm_start': False}
...
```

Use the parameter grids to define the search spaces used in the hyperparameter optimazation.
```
# Parameter Grids for GridSearch

if (GS and model == 'lr'):
    grid = [{'penalty': ['l1', 'l2', 'none'], 
                  'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],

...

# Parameter Grids for HyperOpt

if (HO and model == 'lr'):
    space = {
        'warm_start' : hp.choice('warm_start', [True, False]),
        'fit_intercept' : hp.choice('fit_intercept', [True, False]),
...
```

3.  Functions

Use trainModel to train the final model. The function returns it as crf, as well as test_pred, train_pred as predicitions of the training and test sets
```
def trainModel(classifier,train_features, train_targets, test_features):

    if (GS):
        crf = classifier
    elif (model == 'lr'):
        crf = LogisticRegression(**param).fit(train_features, train_targets)
...
```

Use evaluateModel to evaluate the final model. It prints  ROCs auf the train and test sets, as well as the Confusion Matrix, Sensitivity, Specificity, Precision and F1 Score as evaluaion metris.
```
def evaluateModel(crf, train_targets, train_pred, test_targets, test_pred, test_features):
        
    roc_train = roc_auc_score(train_targets, train_pred)
    roc_test = roc_auc_score(test_targets, test_pred)
...
```

Use GridSearchOpt() to find the best hyperparameters using the GridSearch algorithmn for the choosen model depending on the ROC AUC. Returns the recpective model using cross validation.
```
def GridSearchOpt(classifier, param_grid, train_features, train_targets):
    bestModels = list()

    print('GridSearch for', type(classifier).__name__, ':')   
    
    clf = GridSearchCV(classifier, param_grid, scoring='roc_auc', cv=5, n_jobs=-1, refit =True)
...
```

HyperOptSteps() is used for HyperOpt. It returns the loss and parameters. It will additionally show a progress bar. 
```
def HyperOptSteps(params, model=model, n_fold=10):
    
    # Perform n_fold cross validation with hyperparameters
    # Use early stopping and evaluate based on ROC AUC
    
    if (model == 'lr'):
        clf = LogisticRegression(**params,random_state=0,verbose =0)
...
```
To run HyperOpt use the function:
```
best = fmin(fn = HyperOptSteps, space = space, algo = tpe.suggest, max_evals = 20, trials = Trials())
```
For, at this point, unknow reasons, HyperOpt is unable to return parameter names found by hp.Choice. It will insteadt give back the index number of the dictionary in the predefined parameter grid. At the moment this needs to be manually adjusted before the parameter can be applied in the final model.

4.  Further Steps

To change the train/test split size, change the test_size parameter in the cell #Split Data#:
```
train_features, test_features, train_targets, test_targets = train_test_split(features, target, test_size = 0.25, random_state = 42)
```

5.  Feature importance

The notebook can calculate how important each feature is in the trained model. It will return a list in descending order for Logistic Regression, Random Forest and XGBoost. For LightGBM it will show a plot instead.
```
# Calculate Featute importance

if (model == 'lr'):
    for i in range(0, len(feature_list)):
        crf.coef_[0][i] = '{0:.10f}'.format(abs(crf.coef_[0][i]))
    feature_importances = pd.DataFrame(crf.coef_[0], index = feature_list, columns=['importance']).sort_values('importance', ascending=False)
...
```

6.  Saving the final model

The final model can be saved in a pickle file. 
```
pickle.dump(crf, open(filename, 'wb'))
```
To load the file later use:
```
loaded_model = pickle.load(open("file", 'rb'))
```
